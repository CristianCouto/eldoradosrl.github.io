# Instalaci√≥n del Sistema ü§ñ Chatbot RAG para El Dorado SRL

Este instructivo te guiar√° en la instalaci√≥n y ejecuci√≥n del chatbot RAG (Generaci√≥n Aumentada por Recuperaci√≥n) desarrollado para El Dorado S.R.L.
El chatbot te permitir√° consultar informaci√≥n de la Wiki interna y de documentos PDF, utilizando un modelo de lenguaje grande (LLM) que se ejecuta localmente.

El sistema funciona en una PC local con LM Studio + Backend Flask + Frontend React.

---

## Requisitos del Sistema

### Hardware (Recomendado)

| Componente | Especificaci√≥n sugerida |
|---|---|
| RAM | 16 GB |
| CPU | Ryzen 7 5000 series, Intel i5 o similar |
| GPU | Placa Gr√°fica dedicada |
| OS | Windows 10 / 11 |

### Software (Obligatorio)

* **[Python 3.10+](https://www.python.org/downloads/)**
* **[Node.js 18+](https://nodejs.org/en)** (incluyendo npm)
* **[LM Studio](https://lmstudio.ai/)**

#### Configuraci√≥n del Modelo en LM Studio

1.  **Descargar el Modelo:**
    * Abre LM Studio y usa el buscador (pesta√±a üè†) para encontrar y descargar el modelo recomendado: **`Meta-Llama-3-8B-Instruct-GGUF`**.
    * (Puedes ver detalles de este modelo [aqu√≠](https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF)).

2.  **Configurar el archivo `.env`:**
    * En la ra√≠z del proyecto, encontrar√°s un archivo `.env` que le indica al backend (Python) qu√© modelo debe usar.
    * Las l√≠neas relevantes son:
        ```env
        # Modelo a usar en LM Studio (aseg√∫rate que coincida)
        LLM_MODEL_NAME="Meta-Llama-3-8B-Instruct"
        ```
    * **¬°Importante!** Nota que el nombre en el `.env` (l√≠nea 22) **no** incluye el sufijo `-GGUF`. El script est√° configurado para usar el string `Meta-Llama-3-8B-Instruct`.

#### Opcional: Cambiar de Modelo

Puedes optar por usar un modelo diferente al recomendado.

* **Modelos m√°s potentes (ej. 70B):** Dar√°n respuestas m√°s precisas y coherentes, pero requerir√°n una GPU muy potente y mucha RAM, y ser√°n m√°s lentos.
* **Modelos m√°s peque√±os (ej. 3B):** Ser√°n mucho m√°s r√°pidos y consumir√°n menos recursos, pero la calidad de la respuesta puede ser inferior.

Si descargas un modelo diferente, **debes actualizar la l√≠nea 22** del archivo `.env` para que el valor `LLM_MODEL_NAME` coincida **exactamente** con el "Model Name" que LM Studio espera.

---

## 1. Primera Instalaci√≥n (Configuraci√≥n √önica)

Sigue estos pasos **solo la primera vez** que configures el sistema en una nueva computadora.

### Paso 1.1: Clonar el Repositorio

Abre una terminal (Git Bash, S√≠mbolo del sistema, etc.) y clona el proyecto:

```bash
git clone [https://github.com/santiagooroz-equipo1-pp2-eldorado-2c-2025.git](https://github.com/santiagooroz-equipo1-pp2-eldorado-2c-2025.git)
cd santiagooroz-equipo1-pp2-eldorado-2c-2025
````

### Paso 1.2: Configuraci√≥n del Backend (Ra√≠z)

En la ra√≠z del proyecto, crea un entorno virtual e instala las dependencias de Python.

1.  Crear entorno virtual:

    ```bash
    python -m venv venv
    ```

2.  Activar entorno virtual:

      * En Windows: `venv\Scripts\activate`
      * En macOS/Linux: `source venv/bin/activate`

3.  Instalar dependencias:

    ```bash
    pip install -r requirements.txt
    ```

### Paso 1.3: Configuraci√≥n del Frontend

1.  Navega a la carpeta del frontend:
    ```bash
    cd frontend
    ```
2.  Instala las dependencias de Node.js:
    ```bash
    npm install
    ```
3.  Regresa a la carpeta ra√≠z:
    ```bash
    cd ..
    ```

### Paso 1.4: Indexaci√≥n Inicial (¬°Importante\!)

Antes de iniciar la aplicaci√≥n por primera vez, debes generar la base de datos vectorial (ChromaDB).

  * Aseg√∫rate de estar en la **ra√≠z del proyecto** (`santiagooroz-equipo1-pp2-eldorado-2c-2025`).

  * Aseg√∫rate de que tu **entorno virtual** (`venv`) est√© activado.

  * Ejecuta el script de indexaci√≥n:

    ```bash
    python indexing.py
    ```

Esto leer√° los archivos de `data/pdfs` y la Wiki, generar√° los *embeddings* y los guardar√° en la base de datos vectorial (`data/chroma_db_v...`).

-----

## 2\. Ejecuci√≥n Normal (Uso Diario)

Una vez completada la instalaci√≥n, sigue estos pasos para usar el chatbot.

### Paso 2.1: Iniciar el Servidor de Modelos (LM Studio)

1.  Abre LM Studio.
2.  Ve a la pesta√±a del servidor local (√≠cono `<>`).
3.  Selecciona el modelo que descargaste (ej. `Meta-Llama-3-8B-Instruct-GGUF`).
4.  Haz clic en **Start Server** (Iniciar Servidor).
5.  Aseg√∫rate de que el servidor est√© activo en `http://localhost:1234`.

### Paso 2.2: Iniciar el Backend y Frontend

Hemos incluido scripts para facilitar el inicio simult√°neo del backend (Flask) y el frontend (React).

  * **En Windows:**
    Haz doble clic en el archivo:
    `iniciar_chatbot.bat`

  * **En macOS/Linux:**
    Ejecuta en la terminal:

    ```bash
    chmod +x mac-linux-iniciar.command
    ./mac-linux-iniciar.command
    ```

Esto iniciar√° ambos servicios y abrir√° autom√°ticamente la p√°gina web del chatbot (`http://localhost:3000`) en tu navegador.

 **M√©todo Alternativo (Manual):**
 Si los scripts fallan, puedes iniciar los servicios manualmente en dos terminales separadas (ambas en la ra√≠z del proyecto y con `venv` activado):

 1.  **Terminal 1 (Backend):**
      ```bash
      python app.py
      ```
 

 2.  **Terminal 2 (Frontend):**
     ```bash
     cd frontend
     npm start
     ```

-----

## 3\. Interacci√≥n con el Chatbot

Una vez que la aplicaci√≥n est√© abierta en tu navegador:

### 3.1. Realizar Consultas üí¨

1.  En la parte derecha de la pantalla, ver√°s el panel de chat.
2.  Escribe tu consulta en la burbuja de texto de la parte inferior.
      * **Nota:** La primera consulta puede tardar significativamente m√°s tiempo que las siguientes, ya que el modelo se est√° inicializando.
3.  Presiona el bot√≥n de **Enviar** (la flechita) ‚û°Ô∏è.

### 3.2. Opci√≥n de Voz üéôÔ∏è

1.  Puedes usar el bot√≥n del micr√≥fono üéôÔ∏è a la derecha.
2.  Al presionarlo por primera vez, se te pedir√° permiso para usar el micr√≥fono (selecciona **Aceptar**).
3.  Habla tu consulta.
4.  Vuelve a presionar el bot√≥n del micr√≥fono para finalizar la grabaci√≥n.
5.  Tu voz se transcribir√° en la burbuja de consulta. Puedes editar o a√±adir texto si es necesario.
6.  Presiona **Enviar** ‚û°Ô∏è.

### 3.3. Escuchar Respuesta y Ver Fuentes

1.  **Escuchar Respuesta:** Una vez que el modelo responda, ver√°s un bot√≥n de altavoz üîä debajo de su respuesta. P√∫lsalo para escuchar la respuesta.
2.  **Fuentes:** M√°s abajo, se listar√°n las fuentes utilizadas (PDFs o Wiki) para generar la respuesta.
3.  **Limpiar:** El bot√≥n "Limpiar" üóëÔ∏è elimina el historial de chat de la vista del usuario.

-----

## 4\. Gesti√≥n de Archivos (Administradores)

El panel de la izquierda ("Gesti√≥n de Archivos") permite administrar la informaci√≥n a la que el chatbot tiene acceso. Esta secci√≥n es solo para administradores.

1.  **Acceso al Panel:**

      * Ingresa la clave t√©cnica en el formulario (parte superior izquierda, bajo el logo) y haz clic en "Acceder".

2.  **Administraci√≥n del Conocimiento:**

      * Desde aqu√≠ podr√°s ver los archivos PDF cargados y forzar una re-indexaci√≥n de la base de conocimiento si se realizan cambios.
        
        <img width="549" height="723" alt="Panel de administraci√≥n del chatbot" src="https://github.com/user-attachments/assets/76d72499-26cc-494b-b06c-cb020317ddd7" />
